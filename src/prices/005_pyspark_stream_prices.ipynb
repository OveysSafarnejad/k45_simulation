{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a930739-bb34-41d5-85a3-99845b6dbcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v4/r34w0mtd5xzfzyxp6hnbp8y00000gn/T/ipykernel_18578/3386149659.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, MapType, TimestampType\n",
    "from pyspark.sql.functions import window, col, avg, from_json, from_unixtime, current_timestamp\n",
    "from pyspark.sql.streaming import StreamingQueryListener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "372d8579-762a-43eb-bf3a-0e5be95c7aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your StreamingQueryListener\n",
    "class MyStreamingQueryListener(StreamingQueryListener):\n",
    "    \n",
    "    def __init__(self, first_window_dict):\n",
    "        self.first_window_dict = first_window_dict\n",
    "     \n",
    "    def onQueryStarted(self, query):\n",
    "        self.load_data_from_csv()\n",
    "\n",
    "    def onQueryTerminated(self, query):\n",
    "        ...\n",
    "\n",
    "    def onQueryProgress(self, query):\n",
    "        ...\n",
    "\n",
    "    def onQueryIdle(self, query):\n",
    "        ...\n",
    "\n",
    "    def load_data_from_csv(self):\n",
    "        csv_path = '../shared/output/stream_result.csv'\n",
    "        if os.path.exists(csv_path):\n",
    "            pandas_df = pd.read_csv(csv_path)\n",
    "    \n",
    "            for index, row in pandas_df.iterrows():\n",
    "                group_key = (row['brand'], row['model'], row['year'], row['color'],row[\"mileage\"], row[\"transmission\"], row[\"body_health\"], row[\"engine_health\"], row[\"tires_health\"])\n",
    "                if group_key not in self.first_window_dict:\n",
    "                    self.first_window_dict[group_key] = {\n",
    "                        'brand':row['brand'],\n",
    "                        'model':row['model'],\n",
    "                        'year':row['year'],\n",
    "                        'color':row['color'],\n",
    "                        'mileage':row['mileage'],\n",
    "                        'transmission':row['transmission'],\n",
    "                        'body_health':row['body_health'],\n",
    "                        'engine_health':row['engine_health'],\n",
    "                        'tires_health':row['tires_health'],\n",
    "                        'start': row['start'],\n",
    "                        'end': row['end'],\n",
    "                        'average_price': row['average_price'],\n",
    "                        'processed_at': row['processed_at']\n",
    "                    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec2001fb-7c6c-4d67-a10b-d70017e590a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.5.0/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/hso/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/hso/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-streaming-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ad5b2238-4b61-436f-8fa5-58555c1e5a3b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-10_2.12;3.5.0 in central\n",
      ":: resolution report :: resolve 518ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 by [org.apache.kafka#kafka-clients;3.4.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   1   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ad5b2238-4b61-436f-8fa5-58555c1e5a3b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/7ms)\n",
      "24/02/01 14:59:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "SUBMIT_ARGS = f'--packages ' \\\n",
    "              f'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,' \\\n",
    "              f'org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.0,' \\\n",
    "              f'org.apache.kafka:kafka-clients:2.8.1 ' \\\n",
    "              f'pyspark-shell'\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CarPricesStreaming\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b200673-9cf7-4f75-84a9-af4a82ff84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your first_window_dict\n",
    "first_window_dict = {}\n",
    "\n",
    "# Add the custom listener to Spark session\n",
    "listener = MyStreamingQueryListener(first_window_dict)\n",
    "spark.streams.addListener(listener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ab60433-d4c6-4ec6-bc99-b811aab1d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_message_schema = StructType([\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"model\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"price\", IntegerType(), True),\n",
    "    StructField(\"additional_info\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"ad_publish_time\", IntegerType(), True),\n",
    "    StructField(\"producer_id\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23c86fdb-7276-4290-b898-efa9ce84494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read streaming data from Kafka\n",
    "kafka_bootstrap_server = \"127.0.0.1:9092\"\n",
    "kafka_topic = \"car_prices\"\n",
    "\n",
    "kafka_stream_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_server) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffset\", \"earliest\") \\\n",
    "    .option(\"auto.offset.reset\", \"earliest\") \\\n",
    "    .option(\"includeHeaders\", \"true\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adbb6f96-244a-4af9-9c91-d0e6d6fb0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_prices_raw_df = kafka_stream_df.selectExpr(\"CAST(value AS STRING)\").select(from_json(\"value\", stream_message_schema).alias('car_prices_data'))\n",
    "processed_df = car_prices_raw_df.withColumn(\"ad_publish_time\", from_unixtime(col(\"car_prices_data.ad_publish_time\").cast(\"double\")).cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f33acd2a-fedc-47af-a181-f0dc81d0fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = processed_df.select(\n",
    "    \"car_prices_data.brand\",\n",
    "    \"car_prices_data.model\",\n",
    "    \"car_prices_data.year\",\n",
    "    \"car_prices_data.price\",\n",
    "    \"car_prices_data.additional_info.color\",\n",
    "    \"car_prices_data.additional_info.mileage\",\n",
    "    \"car_prices_data.additional_info.transmission\",\n",
    "    \"car_prices_data.additional_info.body_health\",\n",
    "    \"car_prices_data.additional_info.engine_health\",\n",
    "    \"car_prices_data.additional_info.tires_health\",\n",
    "    \"ad_publish_time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad6fe352-767c-409d-8bb5-24ce5e05c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(df, epoch_id):\n",
    "    global first_window_dict\n",
    "    \n",
    "    pandas_df = df.toPandas()\n",
    "\n",
    "    # Sort the DataFrame by start time\n",
    "    pandas_df = pandas_df.sort_values(by=['start'])\n",
    "\n",
    "    # Get the first row for each group\n",
    "    first_window_df = pandas_df.groupby([\"brand\", \"model\", \"year\", \"color\", \"mileage\", \"transmission\", \"body_health\", \"engine_health\", \"tires_health\"]).first().reset_index()\n",
    "    \n",
    "    # Update the first window information in the global dictionary\n",
    "    for index, row in first_window_df.iterrows():\n",
    "        group_key = (row['brand'], row['model'], row['year'], row['color'],row[\"mileage\"], row[\"transmission\"], row[\"body_health\"], row[\"engine_health\"], row[\"tires_health\"])\n",
    "        first_window_dict[group_key] = {\n",
    "            'brand':row['brand'],\n",
    "            'model':row['model'],\n",
    "            'year':row['year'],\n",
    "            'color':row['color'],\n",
    "            'mileage':row['mileage'],\n",
    "            'transmission':row['transmission'],\n",
    "            'body_health':row['body_health'],\n",
    "            'engine_health':row['engine_health'],\n",
    "            'tires_health':row['tires_health'],\n",
    "            'start': row['start'],\n",
    "            'end': row['end'],\n",
    "            'average_price': row['average_price'],\n",
    "            'processed_at': row['processed_at']\n",
    "        }\n",
    "\n",
    "    # Specify the path to the output CSV file\n",
    "    output_path = \"../shared/output/stream_result.csv\"\n",
    "    \n",
    "    # Write the aggregated data for all groups to the CSV file\n",
    "    first_window_all_groups_df = pd.DataFrame(list(first_window_dict.values()))\n",
    "    first_window_all_groups_df.to_csv(output_path, mode=\"w\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bc8bbf-d615-450d-ad85-eb44463f024e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_duration = \"5 minutes\"\n",
    "slide_duration = \"1 minute\"\n",
    "sliding_window_spec = window(col(\"ad_publish_time\"), window_duration, slide_duration).alias(\"window\")\n",
    "\n",
    "# Define the watermark duration\n",
    "watermark_duration = \"24 hours\"\n",
    "\n",
    "# Define the watermark to only consider data within the last 5 minutes\n",
    "grouped_df = processed_df \\\n",
    "    .withWatermark(\"ad_publish_time\", \"5 minutes\") \\\n",
    "    .groupBy(\"brand\", \"model\", \"year\", \"color\", \"mileage\", \"transmission\", \"body_health\", \"engine_health\", \"tires_health\", sliding_window_spec) \\\n",
    "    .agg(avg(\"price\").alias(\"average_price\"))\n",
    "\n",
    "\n",
    "result_df = grouped_df \\\n",
    "    .select(\"brand\", \"model\", \"year\", \"color\", \"mileage\", \"transmission\", \"body_health\", \"engine_health\", \"tires_health\", \"window.start\", \"window.end\", \"average_price\")\n",
    "\n",
    "result_df = result_df.withColumn(\"processed_at\", current_timestamp().cast(TimestampType()))\n",
    "\n",
    "# Start the streaming query\n",
    "query = result_df.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .foreachBatch(write_to_csv) \\\n",
    "    .option(\"checkpointLocation\", \"./checkpoints\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the streaming query to terminate\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
